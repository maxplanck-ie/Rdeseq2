---
title: "Rcourse_p2"
author: "Mirela Balan"
date: "`r date() `"
output:
  html_document:
    code_folding: hide
    toc: true
    toc_depth: 2
    toc_float: true
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
.libPaths("/data/manke/processing/RdeseqEnv/R-4.0.3_Rcourse/lib64/R/library")
setwd("~/Desktop/Rdeseq2/rmd/")
```

#### **Load libraries**
```{r paths_libraries, eval=TRUE, message=FALSE}
library(tidyverse)
library(DESeq2)
library(apeglm)
#library(dplyr)
library(ggplot2)
library(pheatmap)
library(knitr)
```


## **1. Quick recap** - 50 min
The steps required to do differential expression analysis using DESeq2 package (with default parameters):  NOT RUN

```{r recap, eval=FALSE}
# make the DESeqDataSet (dds) object:
dds <- DESeqDataSetFromMatrix(countData = matrix_with_count_data,
                              colData = dataframe_with_sample_information,
                              design = what_variables_interest_you)

# explore data & QC: transform data, plot (PCA, heatmap)

# perform statistical analysis 
dds <- DESeq(dds)

# extract results
res <- results(dds)
```

Things are a bit more complicated in reality - an interative process.
<br>
<center>
![Workflow for DESeq2](../images/workflow_DESeq2.png){width=100%}
</center>
<br>


### **1.1. Load data, metadata, decide on design**
You have learned more about loading and fixing data, exploring it and creating a DESeq2 object in the previous day. Here the same steps are summarized and used to load the data for today's lesson.

```{r load_data_metadata, message=FALSE}
#Loading data and metadata (sample sheet)
data <- read.table("../data/qc/counts_qc.tsv", header=TRUE, check.names = TRUE)
metadata <- read.table("../data/qc/counts_qc_meta.tsv", header=TRUE, check.names = TRUE, stringsAsFactor = F)
colnames(metadata) <- c("sample", "celltype", "condition")

metadata$celltype <- as.factor(metadata$celltype)    # observe the change from 'character' to 'factor'
metadata$condition <- as.factor(metadata$condition)
rownames(metadata) <- metadata$sample  
head(data)
head(metadata)

#check if the col and rows are in the same order:
all(rownames(metadata) == colnames(data))
```

#### **1.1.a. count data**
- usually named 'data', 'counts'
- a matrix with **raw counts** - not normalized, not transformed; integers
- to explore: head(data), dim(data), str(data), summary(data)
  
#### **1.1.b. information about the samples**
- usually named 'metadata', 'colData'
- dataframe with all known useful information about the samples: name, condition (control/treatment, WT/KO/KD/OE), batch/ sequencing day, gender, age
- to explore: head(metadata)
  - rows of the metadata should be named with sample names (important for downstream steps)
  - **column names of the data should be identical and in the same order with the row names of the metadata!**: `all(colnames(data)==rownames(metadata))` should output 'TRUE'
- factorization!
  - used to represent categorical data 
  - stored as integers that have associated labels: R sorts the labels alphabetically


        1    2    3
        AMIL DMSO TG
  - can be: 
    - ordered: small < normal < large; sand < pebels < stones < boulders;
    - unordered: DMSO, TG, AMIL; WT, KO-1, KO-2;
  - levels = predefined factors; reference level = the 1st level
  - the order in which the factors are ordered matters: 
      - for plotting:
```{r bar1, echo=FALSE, out.width = '60%', fig.align="center"}
par(mfrow = c(1,2))

my_numbers1 <- c(25, 12, 34)
my_names1 <- c('high', 'low', 'medium')
barplot(my_numbers1, names.arg = my_names1, main = 'Not what you want')

my_numbers2 <- c(12, 34, 25)
my_names2 <- c('low', 'medium', 'high')
barplot(my_numbers2, names.arg=my_names2, main = "The correct display")
par(mfrow = c(1,1))
```
      - for statistics: for DE analysis, DESeq2 compares all other levels within a variable with the 1st level. So if they are mixed, instead of comparing 'treatment1 to control' and 'treatment2 to control', you might end up with unexpected results! 

<br>
<center>
![Order of factors influences later comparisons](../images/factor_order_comparison.png){width=60%}
</center>
<br>



  - how to reorder factors:    NOT RUN
```{r reorder_factors, eval=FALSE}
# in the metadata dataframe
metadata$condition <- factor(metadata$condition, levels = c ('DMSO', 'AMIL', 'TG'))

# in the dds object
dds$condition <- relevel(dds$condition, ref='DMSO')   # if you only care about what is the reference level

# or
dds$condition <- factor(dds$condition, levels = c ('DMSO', 'AMIL', 'TG'))  # if you want a specific order for all of them

# not reorder your factors, but then you have to use the 'contrast' arguments in the results() function 
```


Back to our data. This is the order of the factors before and after re-arrangement:
```{r rearrange_factors}
#initial factor arrangement:
metadata$condition

#rearrange factors
metadata$condition <- factor(metadata$condition, levels = c("DMSO", "AMIL", "TG"))

metadata$condition
```


#### **1.1.c. experimental design**
- contains variables which will be used in data modeling for estimating dispersions and the log fold changes.
- formula:
  - starts with a tilde '~'
  - the names used are the names of the columns of the metadata
  - add the needed variables, with the one you are most interested in explaining at the end
    - e.g. '~ batch + condition' = measure the effect of condition, while controlling for batch-induced differences
  - make sure the control condition (e.g. DMSO, WT, PBS) is the reference level

```{r design}
design <- ~ celltype + condition
```

All that is left now is to make the dds object:

```{r ddsObj1}
dds_old <- DESeqDataSetFromMatrix(countData=data, colData=metadata, design=design) 
```
- dds object stores:
  - read counts (data) - call it with `counts(dds)`
  - sample info (metadata) - call it with `colData(dds)`
  - intermediate estimated quantities - during statistical analysis
  - more additional features that you can add manually to the metadata columns of a newly constructed object: `mcols(dds) <- DataFrame(mcols(dds), my_dataframe)`
  

### **1.2. Exploring data - part 1 **
Previously you noticed that the PCA shows a problem with our data. The dataset you are working on has been changed for the purpose of teaching you data exploration. But you might find this kind of issue in datasets without them being tampered.

```{r old_PCA, echo=FALSE, fig.align="center"}
#plotPCA is the DESeq2 function to plot automatically your samples on a PCA. However, the images don't look to nice. So we use it to extract the values needed for plotting and then using them with ggplot to get a better PCA.
rld_old <- plotPCA(rlog(dds_old, blind = TRUE), intgroup=c("condition", "celltype"), returnData=TRUE)
percentVar <- round(100 * attr(rld_old, "percentVar"), 1)   # rounded values for each axis of PC variance

ggplot(rld_old, aes(PC1, PC2, color=condition, shape=celltype)) +
  geom_point(size=3) +
  xlab(paste0("PC1: ",percentVar[1])) +
  ylab(paste0("PC2: ",percentVar[2])) 
```

Let's fix the dataset, to return it to the way the data was originally published:
```{r fix_data, results='hide', fig.keep='all', fig.align="center"}
#Fix data:
#switch rows 1 and 7 in metadata
metadata <- metadata[c(7, 2:6, 1, 8:nrow(metadata)),]
rownames(metadata) <- metadata$sample    # needed for plotting with pheatmap
#metadata$sample <- NULL    #if you want to remove the 'sample' column

#switch column names in counts:
colnames(data)[c(1,7)] <- c("BM_CTRL_1", "JJ_CTRL_1")

#check if the col and rows are in the same order:
all(rownames(metadata) == colnames(data))

#new PCA
design <- ~ celltype + condition        #!!! names must be the same as the columns in the metadata file!!!
dds <- DESeqDataSetFromMatrix(countData=data, colData=metadata, design=design) 
rld <- plotPCA(rlog(dds, blind = TRUE), intgroup=c("condition", "celltype"), returnData=TRUE)  # 'plotPCA' is a DESeq2 function used for plotting transformed data; can use ggplot code you learned yesterday
percentVar <- round(100 * attr(rld, "percentVar"), 1)   # rounded values for each axis of PC variance 

ggplot(rld, aes(PC1, PC2, color=condition, shape=celltype)) +
  geom_point(size=3) +
  xlab(paste0("PC1: ",percentVar[1])) +
  ylab(paste0("PC2: ",percentVar[2])) 

```
You can see that now the samples group nicely on the PCA.


**Q1**: What would you do if you find yourself in the situation where there is something wrong in your dataset? Would you drop the responsible sample? Would you modify the dataset? How would you make that choice?




## **2. Data exploration, visualization and QC**   - 30 min
After creating the DESeq2 object, you need to explore your data further to learn more about it, to check if your expectations are met. Also, the downstream analyses rely on a couple of assumptions and we need to see to what degree our data match those assumptions, and to correct it if possible. Also, to create informative, easy to interpret visualizations, the data needs to be transformed before plotting. We need to talk about:
- filtering
- distributions for data modeling: mean, stdvar, variance, dispersion, overdispersion
- transformations: log, vst, rlog


### **2.1. Data pre-filtering**
Let's look at our data using the 'summary()' function:
```{r summary_data}
summary(counts(dds))[,1:3]  #shows the summary only for the first 3 samples - for brevity
```
**Q2**: notice the output - what do the various parameters tell you about the data? What does the 'mean' tell you, practically?

- several types of filtering, some manual others automatic
- the pre-filtering is helpful but not necessary to remove rows/genes that have very low counts (very unlikely to have biological meaning)
  - arbitrary threshold: < 1, < 10
  - reduces the memory size that the 'dds' object occupies => increases the speed of transformations and calculations
  - reduces the numbers of comparisons done when testing for significance => improves the outcomes

```{r DESeq2_filtering, fig.align="center"}
#decide on the threshold you want to use  (= the minimum number of counts you want to keep)
keep <- rowSums(counts(dds)) > 1
cat('genes before filter: ', nrow(dds), '\n')    # prints the number of rows you have before the filtering

# keep the desired rows
fdds <- dds[keep, ]                               # most people keep the 'dds' name, but I will change it to help you differentiate between the different steps and learn. But doing this will fill up your memory.
cat('genes before filter: ', nrow(fdds), '\n')    # prints the number of rows you have after the filtering
```

Notice how many rows you have dropped. In this case you are left only with **`r round(nrow(fdds) * 100 / nrow(dds), digits=2)` %** of the initial number of rows!


NOTE1: the threshold depends on the dataset, your goals and the R package you use. With DESeq2 it is recommended to only do a minimal filtering to remove the zero counts. This is because the later analysis will apply their own automatic filtering, such as when using the 'results()' function from DESeq2.

NOTE2: If you have an unequal number of samples in the groups, you should set the minimal number of samples in which the counts should be above a certain value. For example, if your smallest group has only 5 samples and the others more, and you want a stricter threshold (of lets say 10 counts), use the filtering as such: `keep <- rowSums(counts(dds) >= 10) >= 5`

### **2.2. Modeling count data**
An important step in DE analysis is to model the association between gene counts and covariates of interest = find the (cluster of) genes that significantly change their expression, that can be attributed to a combination of factors outlined in your experimental design.

- data can be modeled with various distributions depending on their properties:

- terms:
  - mean of the group: the average value of a sample
  - variance: the expectation of the squared deviation (stdev^2) of a variable from its population mean. It is a measure of dispersion (of how far a set of numbers is spread out from their average value)

```{r normal_distribution}
x <- seq(-4, 4, length=200)
y <- dnorm(x, mean=0, sd=1)

plot(x, y, type="l", lwd=2,  axes=FALSE, ann = FALSE)  #  scatterplot with connected lines (type = "l"); remove axes and the axes titles
title ("Normal distribution (mean = 0, stdev=1)", cex=0.8)

#choose the colors for the plot:
custom_grey_pallete <- colorRampPalette(c("grey0", "grey100"))
my_colors <- custom_grey_pallete(5)

i <- x >= -3 & x <= 3
#lines(x, y)
polygon(c(-3,x[i],3), c(0,y[i],0), col=my_colors[4])

i <- x >= -2 & x <= 2
polygon(c(-2,x[i],2), c(0,y[i],0), col=my_colors[3])

i <- x >= -1 & x <= 1
polygon(c(-1,x[i],1), c(0,y[i],0), col=my_colors[2])

abline(v = 0, lty=2, col='white')

# Display text on the plot. The default "side" parameter is 3, representing the top of the plot.
mtext("Mean (μ)", side = 3, line=-0.5)               


# add multiple labels to the axis
mtext('Values\nStdev from mean\n Z-score', side =1,
      line = 3,
      at = -3.5, adj=1, cex=0.9)

axis(1, at=seq(-3, 3, by=1), labels=c("μ-3σ", "μ-2σ", "μ-1σ", "μ", "μ+1σ", "μ+2σ", "μ+3σ"), cex=0.9)  
axis(1, at=seq(-3, 3, by=1), labels=c('-3σ', '-2σ', '-1σ', '0', '1σ', '2σ', '3σ'), line=1, tick = FALSE, cex=0.9)
axis(1, at=seq(-3, 3, by=1), labels=c('-3.0', '-2.0', '-1.0', '0', '1.0', '2.0', '3.0'), line=2, tick = FALSE, cex=0.9)

### remember: value of z-score: z=(value-mean)/sigma
```

```{r var}
x <- seq(-8, 8, length=200)
y <- dnorm(x, mean=0, sd=1)
z <- dnorm(x, mean=0, sd=3)

plot(x, y, type="l", lwd=2,  axes=FALSE, ann = FALSE )  #  scatterplot with connected lines (type = "l"); remove axes and the axes titles
title ("Normal distribution (mean = 0)", cex=0.8)
lines(x,z,col="green")

#legend("topright", legend=c('stdev=1', 'stdev=3'), col=c('black', 'green'))

abline(v = 0, lty=2, col='black')
abline(v = 0, lty=1, col='green')
text(2, 0.3, labels="stdev = 1", col = 'black')
text(3, 0.12, labels="stdev = 3", col = 'green')
```
- RNAseq data is:
  - discrete (counts)
  - very large number of RNAs are represented and the probability of pulling out a particular transcript is very small
    - Poisson? (assumes that mean = variance)
    - Binomial? (assumes that mean > variance)
  
```{r mean_vs_variation, warning=FALSE}
#overdispersion
mean_counts <- apply(counts(dds)[,6:8], 1, mean)        # '1' means that the function 'apply' is applied to rows. Use '2' if applied to columns 
variance_counts <- apply(counts(dds)[,6:8], 1, var)
df <- data.frame(mean_counts, variance_counts)

ggplot(df) +
        geom_point(aes(x=mean_counts, y=variance_counts)) + 
        scale_y_log10(limits = c(1,1e9)) +
        scale_x_log10(limits = c(1,1e9)) +
        geom_abline(intercept = 0, slope = 1, color="red")
```
We observe from above that mean < variance => we should use a **negative binomial** distribution

```{r negative_binomial_distribution}
x <- 0:30

plot(x, dnbinom(x, size=0.5, mu=10), type = "l", col='red')
lines(x, dnbinom(x, size=1, mu=10), type = "l", col='orange')
lines(x, dnbinom(x, size=2, mu=10), type = "l", col='green')
lines(x, dnbinom(x, size=5, mu=10), type = "l", col='blue')
lines(x, dnbinom(x, size=10, mu=10), type = "l", col='purple')
lines(x, dnbinom(x, size=15, mu=10), type = "l", col='black')
lines(x, dnbinom(x, size=20, mu=10), type = "l", col='pink')

title ("Negative binomial distribution (mean = 10)", cex=0.8)
text(2, 0.3, labels="stdev = 1", col = 'black')
text(3, 0.12, labels="stdev = 3", col = 'green')
```



<!-- - for genes with high mean expression, the variance across replicates tends to be greater that the mean -->
<!-- - for genes with low mean expression, you see quite a bit of scatter = hetero-scedasti-city = for a given expression level in the low range, we obs a lot of variability in the variance values. -->

### **2.3. Data transformation**
DESeq2 needs as input **raw counts** for statistical testing, but for some stages of visualization or downstream analysis, you need **transformed data**. Methods of exploratory analysis of multidimensional data work best for data that has in general the same range of variance at different ranges of the mean values (also called **homoskedastic data**). We just saw that mRNA data is **heteroskedastic** (variance grows with the mean), so if we plot that data, the biggest difference will be given by the genes with the highest counts because they show the largest absolute differences between samples. 

a) logarithmic data - also called **pseudocounts**; used mostly in visualization because of the large dynamic range of the data; the logarithmic function is of the type: log2(count+n0), usually log2(count+1). The 'n0' value is a positive constant that is introduced because of the zero counts (log(0) tends to infinity and this introduces other issues in the visualization). The problem is that the genes with the very lowest counts (which also have a very poor noise-to-signal ratio) contribute the greatest amount the noise (log of small numbers inflates their variance) = **the low counts genes will overly contribute to sample-sample distances**.
We need to stabilize the variance across the mean, for which we can use:

```{r log}
print('dataset with counts before transformation (raw counts):' )  
head(counts(fdds),3)    

print('dataset with counts after logarithmic transformation:' )  
head(log(counts(fdds), 2) ,3) 
```

b) variance stabilizing transformations (VTS) - for negative binomial data with a dispersion-mean trend. Faster and much less sensitive to high count outliers that 'rlog' transformation. Use for medium-large datasets (n>30). On the plot, you can see an upward shift for the smaller values

```{r vst}
print('dataset with counts after VTS transformation:' )  
vsd <- vst(fdds, blind = FALSE)  # 'blind=FALSE' means that differences between cell lines and treatment (the variables in the design) will not contribute to the expected variance-mean trend of the experiment. The experimental design is not used directly in the transformation, only in estimating the global amount of variability in the counts. For a fully unsupervised transformation, one can set blind = TRUE
head(assay(vsd),3)
```

c) regularized logarithm (RLOG) - slower because it requires fitting a shrinkage term for each sample and each gene. Works well for small datasets (n < 30) and when there is a wide range of sequencing depth across samples (an order of magnitude difference).

```{r rlog}
print('dataset with counts after RLOG transformation:' )  
rld <- rlog(fdds, blind = FALSE)
head(assay(rld),3)
```
- b) and c) have similar properties. For high counts, the results are similar to the logarithmic transformation. But for low counts, the values are shrunken towards a middle value. This makes the data behave more like homoskedastic data. We can see the differences in plots. . Sequencing depth correction is done automatically for the vst and rlog.
```{r diff_log_vst_rlog}

dds <- estimateSizeFactors(dds) # needed for log2 approach to account for sequencing depth

df <- bind_rows(
  as_tibble(log2(counts(dds, normalized=TRUE)[, 1:2]+1)) %>%
         mutate(transformation = "log2(x + 1)"),
  as_tibble(assay(vsd)[, 1:2]) %>% mutate(transformation = "vst"),          # normalization and size factors not needed - automatically detected
  as_tibble(assay(rld)[, 1:2]) %>% mutate(transformation = "rlog"))
  
colnames(df)[1:2] <- c("x", "y")  

lvls <- c("log2(x + 1)", "vst", "rlog")
df$transformation <- factor(df$transformation, levels=lvls)

ggplot(df, aes(x = x, y = y)) + geom_bin2d(bins=80) +
  coord_fixed() + facet_grid( . ~ transformation) 
```


Does it make a difference to the visualizations?
```{r ploc}
# th <- data.frame(round(log(counts(fdds)+1)))
# 
# ldds <- DESeqDataSetFromMatrix(countData=th, colData=metadata, design=design) 

th <- data.frame(round(log(counts(fdds)+1)))

th.pca <- prcomp(t(th))
pca_data_perc=round(100*th.pca$sdev^2/sum(th.pca$sdev^2),1)

df_pca_data=data.frame(PC1 = th.pca$x[,1], PC2 = th.pca$x[,2], sample = metadata$celltype, condition=metadata$celltype)  #! change condition accordingly!


ggplot(df_pca_data, aes(PC1, PC2, color=sample)) +
  geom_point(size=3) +
  xlab(paste0("PC1: ",pca_data_perc[1])) +
  ylab(paste0("PC2: ",pca_data_perc[2])) 

#############################
rld_new2 <- plotPCA(vst(fdds, blind = TRUE), intgroup='celltype', returnData=TRUE)  # 'plotPCA' is a DESeq2 function used for plotting transformed data; can use ggplot code you learned yesterday
percentVar <- round(100 * attr(rld_new2, "percentVar"), 1)   # rounded values for each axis of PC variance 

ggplot(rld_new2, aes(PC1, PC2, color=celltype)) +
  geom_point(size=3) +
  xlab(paste0("PC1: ",percentVar[1])) +
  ylab(paste0("PC2: ",percentVar[2])) 

#############################
rld_new3 <- plotPCA(rlog(fdds, blind = TRUE), intgroup='celltype', returnData=TRUE)  # 'plotPCA' is a DESeq2 function used for plotting transformed data; can use ggplot code you learned yesterday
percentVar <- round(100 * attr(rld_new3, "percentVar"), 1)   # rounded values for each axis of PC variance 

ggplot(rld_new3, aes(PC1, PC2, color=celltype)) +
  geom_point(size=3) +
  xlab(paste0("PC1: ",percentVar[1])) +
  ylab(paste0("PC2: ",percentVar[2])) 

##ref: https://www.biostars.org/p/289333/
```






## **3. Differential expression analysis**  - 30 min
The results can be returned using the standard function:
```{r dds_DE, eval=FALSE}
dds <- DESeq(dds)
```
There are several steps to generating the DE results, and they are wrapped together in the **DESeq()** function. The 3 main steps are estimations of:
- estimating size factors
- estimating dispersions
- fitting model and testing


### **3.1. Size factors**
The libraries have different sizes, so when using data for statistical analysis, you must have comparable libraries for the downstream estimations = normalization. The size factors are meant to account for the different sequencing depths.
```{r size_libraries}
colSums(counts(dds))
```

'DESeq()' function estimates size factors automatically. But if you run `estimateSizeFactors()` before the `DESeq()` functions, these values would be used instead.
```{r size_factors}
dds <- estimateSizeFactors(dds)
sizeFactors(dds)
```
- the larger size factors correspond to the samples with higher sequencing depth. 
- to generate the normalized counts we need to divide the counts by the size factors. 

Now, see the changes in the values of the library sizes:
```{r normalized}
colSums(counts(dds, normalized=T))
```



### **3.2. Dispersions**
DESeq2 measures variation using the term 'dispersion', that actually connects a gene’s variance and mean expression level. Dispersion is calculated by `Var = μ + α*μ^2`, where `α` = dispersion, `Var` = variance, and `μ` = mean

```{r estimate dispersions, message=FALSE, warning=FALSE}
dds <- estimateDispersions(dds)
```

DESeq2 has a function that allows to visualize the dispersion in your analysis
```{r Visualise dispersions, message=FALSE, warning=FALSE}
plotDispEsts(dds)
```


- gene-wise dispersion by fitting a negative binomial general linear model (GLM) to the data and then returning the results using Wald statistics (**nbinomWaldTest**)
```{r dds_DE2, warning=FALSE, message=FALSE}
dds <- DESeq(dds)
```

## **4. Extracting results** - 10 min
To see the results, use the **results()** function which gives you the **log2 fold changes** and the **adjusted p-values**, for the last variable in the design formula (the estimates are of the logarithmic fold change log2(treated/untreated). In our case it will AMIL/DMSO and TG/DMSO).

```{r run_DESeq2, message=FALSE}
dds_res <- results(dds)
dds_res
```

